{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa037dc2-9be0-4c75-b65d-cd7a7153044c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 1: Create Silver Schema and Volume\n",
    "\n",
    "- üóÇÔ∏è Creates the **`silver_ecommerce`** schema within the catalog to **store cleaned and enriched data**.\n",
    "- üì¶ Creates a **volume** inside the Silver schema to **hold intermediate files**, typically transformed from raw Bronze data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c172717-3ddd-4206-a3cf-9e34bbe1ec44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the schema if it doesn't exist\n",
    "CREATE SCHEMA IF NOT EXISTS dataengineer_databricks_ecommerce.silver_ecommerce;\n",
    "\n",
    "-- Create a volume inside the schema\n",
    "CREATE VOLUME IF NOT EXISTS dataengineer_databricks_ecommerce.silver_ecommerce.silver_volume;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b92241e-389f-4f58-b400-337ab7be977f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 2: Load Data from Bronze Volume with Schema Inference\n",
    "\n",
    "- üì• Loads all CSV files from the **Bronze volume** using Spark with header recognition and automatic schema inference.\n",
    "- üßæ Uses `display(df)` to **visually inspect the structured DataFrame**, making it easier to understand the data types and contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9aa784f-ec7b-42cf-9b88-29692b8f0707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/Volumes/dataengineer_databricks_ecommerce/bronze_ecommerce/bronze_volume\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e5cf6e-1f36-41ac-99c8-666c462d7806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2bd99f-6dc0-4b5e-8811-b74948685bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "561b0a46-5b94-42b0-bdcc-06938c6941f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 3: Rename Columns (Optional but Recommended)\n",
    "\n",
    "- üßπ Standardizes column names by **removing spaces, special characters, or inconsistent casing**.\n",
    "- ‚úÖ Improves **readability, consistency, and compatibility** when performing transformations or writing SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7b55b3-bcf3-4610-b49e-0c9c39fa6cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumnRenamed(\"Assigned Supervisor\",\"Assigned_Supervisor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b678d61-68e1-45d1-b3fe-eab7db5448c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 4: Handle Nulls and Missing Values\n",
    "\n",
    "- üö´ **Filter out rows** where any column contains null values, if completeness is critical.\n",
    "- ‚ö†Ô∏è **Drop rows** with nulls in critical fields like `Order_Number` or `Order_Date` to maintain data integrity.\n",
    "- üõ†Ô∏è **Fill or flag** non-critical nulls using default values, imputation, or indicators for further review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47abb7d-706c-498d-b5d0-23d2f2868b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter rows where any column is null\n",
    "df_na = df.filter(\n",
    "    col(\"Order_Number\").isNull() |\n",
    "    col(\"State_Code\").isNull() |\n",
    "    col(\"Customer_Name\").isNull() |\n",
    "    col(\"Order_Date\").isNull() |\n",
    "    col(\"Status\").isNull() |\n",
    "    col(\"Product\").isNull() |\n",
    "    col(\"Category\").isNull() |\n",
    "    col(\"Brand\").isNull() |\n",
    "    col(\"Cost\").isNull() |\n",
    "    col(\"Sales\").isNull() |\n",
    "    col(\"Quantity\").isNull() |\n",
    "    col(\"Total_Cost\").isNull() |\n",
    "    col(\"Total_Sales\").isNull() |\n",
    "    col(\"Assigned Supervisor\").isNull()\n",
    ")\n",
    "\n",
    "df_na.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5334816-79fd-4e27-8797-004752710fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.dropna(subset=[\"Order_Number\",\"Order_Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d29fb2-789d-43a6-9c5f-6f5c74706a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter rows where any column is null\n",
    "df_na = df.filter(\n",
    "    col(\"Order_Number\").isNull() |\n",
    "    col(\"State_Code\").isNull() |\n",
    "    col(\"Customer_Name\").isNull() |\n",
    "    col(\"Order_Date\").isNull() |\n",
    "    col(\"Status\").isNull() |\n",
    "    col(\"Product\").isNull() |\n",
    "    col(\"Category\").isNull() |\n",
    "    col(\"Brand\").isNull() |\n",
    "    col(\"Cost\").isNull() |\n",
    "    col(\"Sales\").isNull() |\n",
    "    col(\"Quantity\").isNull() |\n",
    "    col(\"Total_Cost\").isNull() |\n",
    "    col(\"Total_Sales\").isNull() |\n",
    "    col(\"Assigned Supervisor\").isNull()\n",
    ")\n",
    "\n",
    "# Display the filtered DataFrame containing rows with at least one null value\n",
    "df_na.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a69637-636d-43e8-8611-42d72562fe3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows where the 'Assigned_Supervisor' column has the value 'unassigned'\n",
    "df_unassigned = df.filter(df[\"Assigned_Supervisor\"] != \"unassigned\")\n",
    "\n",
    "# Display the filtered DataFrame in the Databricks notebook\n",
    "display(df_unassigned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c422f8a-ea1a-4b2e-a3ed-9492366892e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 5: Convert Data Types (if needed)\n",
    "\n",
    "- üî¢ Ensure that **numeric fields** (e.g., prices, quantities) are properly typed as integers or floats.\n",
    "- üß™ Helps avoid **calculation errors** and improves performance during aggregations and transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e1562b-f87a-4ce0-b115-9c1b23bf098d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"Cost\", col(\"Cost\").cast(\"double\")) \\\n",
    "       .withColumn(\"Sales\", col(\"Sales\").cast(\"double\")) \\\n",
    "       .withColumn(\"Quantity\", col(\"Quantity\").cast(\"int\")) \\\n",
    "       .withColumn(\"Total_Cost\", col(\"Total_Cost\").cast(\"double\")) \\\n",
    "       .withColumn(\"Total_Sales\", col(\"Total_Sales\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ddab040-67a9-472d-ae61-bcc6aaa85acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Step 6: Add Derived Columns\n",
    "\n",
    "- ‚ûï Add **calculated fields** such as `profit` (e.g., revenue - cost) to enrich the dataset with business insights.\n",
    "- üìÖ Include **dynamic columns** like today‚Äôs date to track data processing time or for audit purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fdab84-e797-4ed9-ba96-0def97a1ecd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr,current_date\n",
    "\n",
    "df = df.withColumn(\"Profit\", expr(\"Total_Sales - Total_Cost\"))\n",
    "df_silver = df.withColumn(\"Load_Date\", current_date())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "817067a1-dc0c-4881-933c-e710d1fdc00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 7: Save DataFrame as a CSV File to the Volume\n",
    "\n",
    "- üíæ Save the transformed DataFrame (e.g., `df_silver`) as a **CSV file** to the Silver volume for downstream processing or analysis.\n",
    "- üìÇ This step ensures the **cleaned and enriched data** is persisted in a structured format within Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624bde99-4509-4154-a48e-a4d50934ccc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "    \"/Volumes/dataengineer_databricks_ecommerce/silver_ecommerce/silver_volume/orders_cleaned\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8323729221073883,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_ecommerce",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
